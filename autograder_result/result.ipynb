{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f64853b-a8dc-4fb4-9917-29649e1a4d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:29:41.604552Z",
     "iopub.status.busy": "2023-11-22T05:29:41.603776Z",
     "iopub.status.idle": "2023-11-22T05:29:44.382217Z",
     "shell.execute_reply": "2023-11-22T05:29:44.380969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\r\n",
      "=======================\r\n",
      "Status=Up/Down\r\n",
      "|/ State=Normal/Leaving/Joining/Moving\r\n",
      "--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \r\n",
      "UN  172.28.0.2  70.26 KiB  16      74.5%             19575424-6d89-4092-8620-c7d3b33225e5  rack1\r\n",
      "UN  172.28.0.4  70.26 KiB  16      59.1%             35a51a44-83e8-483b-8d3d-37097f2de251  rack1\r\n",
      "UN  172.28.0.3  70.26 KiB  16      66.3%             8ff44324-ec21-471a-a971-1fe334012782  rack1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f3512-ed21-479e-8805-aa384a4032c8",
   "metadata": {},
   "source": [
    "## Part 1: Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fed97e7-961f-4f1d-857e-fab5be6f2c47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:29:44.387275Z",
     "iopub.status.busy": "2023-11-22T05:29:44.386919Z",
     "iopub.status.idle": "2023-11-22T05:29:45.088595Z",
     "shell.execute_reply": "2023-11-22T05:29:45.086957Z"
    }
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4b5280-5ced-407b-8f43-841245860290",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:29:45.099004Z",
     "iopub.status.busy": "2023-11-22T05:29:45.096557Z",
     "iopub.status.idle": "2023-11-22T05:29:49.879617Z",
     "shell.execute_reply": "2023-11-22T05:29:49.878080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f59f3ba9510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"drop keyspace if exists weather\")\n",
    "cass.execute(\"\"\"create keyspace weather with replication = {\n",
    "    'class' : 'SimpleStrategy',\n",
    "    'replication_factor' : 3\n",
    "    };\n",
    "\"\"\")\n",
    "cass.execute(\"use weather\")\n",
    "cass.execute(\"\"\"\n",
    "    create type station_record(\n",
    "        tmin INT,\n",
    "        tmax INT\n",
    "    )\n",
    "\"\"\")\n",
    "cass.execute(\"\"\"\n",
    "    create table stations(\n",
    "        id TEXT,\n",
    "        name TEXT STATIC,\n",
    "        date DATE,\n",
    "        record WEATHER.STATION_RECORD,\n",
    "        PRIMARY KEY ((id), date)\n",
    "    ) WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c06cdb-e25f-4783-b8ee-060bf82d170d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:29:49.897183Z",
     "iopub.status.busy": "2023-11-22T05:29:49.894767Z",
     "iopub.status.idle": "2023-11-22T05:29:49.947534Z",
     "shell.execute_reply": "2023-11-22T05:29:49.946413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "cass.execute(\"describe table weather.stations\").one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b38dee-19fb-47e5-a3f0-735c02ee2c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:29:49.954438Z",
     "iopub.status.busy": "2023-11-22T05:29:49.953687Z",
     "iopub.status.idle": "2023-11-22T05:30:05.983381Z",
     "shell.execute_reply": "2023-11-22T05:30:05.981062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-19c51744-5b5b-4198-bd17-3033d8716667;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.typesafe#config;1.4.1 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (137ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (76ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (339ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (51ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (27ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (143ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (37ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (101ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (33ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (33ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (26ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (35ms)\n",
      ":: resolution report :: resolve 5631ms :: artifacts dl 1256ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-19c51744-5b5b-4198-bd17-3033d8716667\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/120ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 05:30:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b531f4-8cc4-4065-b9ae-d47591a93994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:30:05.990304Z",
     "iopub.status.busy": "2023-11-22T05:30:05.989568Z",
     "iopub.status.idle": "2023-11-22T05:30:33.624897Z",
     "shell.execute_reply": "2023-11-22T05:30:33.620546Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 2]\r",
      "\r",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                  (0 + 2) / 2][Stage 1:>                  (0 + 0) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>    (0 + 2) / 2][Stage 1:>    (0 + 0) / 2][Stage 2:>    (0 + 0) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>    (0 + 2) / 2][Stage 1:>    (0 + 1) / 2][Stage 2:>    (0 + 0) / 2]\r",
      "\r",
      "[Stage 1:>                  (0 + 2) / 2][Stage 2:>                  (0 + 0) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                  (0 + 2) / 2][Stage 2:>                  (0 + 1) / 2]\r",
      "\r",
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "import pandas as pd\n",
    "df = spark.read.text(\"ghcnd-stations.txt\")\n",
    "ID_df = df.withColumn(\"ID\", expr(\"substring(value, 0, 11)\"))\n",
    "STATE_df = df.withColumn(\"STATE\", expr(\"substring(value, 39, 2)\"))\n",
    "NAME_df = df.withColumn(\"NAME\", expr(\"substring(value, 42, 30)\"))\n",
    "df2 = ID_df.join(STATE_df, ['value'], how='inner')\n",
    "df2 = df2.join(NAME_df, ['value'], how='inner')\n",
    "filtered_df = df2.where(col(\"STATE\") == \"WI\")\n",
    "\n",
    "df_insert = cass.prepare(\"\"\"\n",
    "    INSERT INTO stations (id, name)\n",
    "    VALUES (?, ?)\n",
    "\"\"\")\n",
    "\n",
    "filtered_df = filtered_df.toPandas()\n",
    "for index, row in filtered_df.iterrows():\n",
    "    cass.execute(df_insert, (filtered_df[\"ID\"][index], filtered_df[\"NAME\"][index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8114a52-01a0-409e-abd1-040090aefa98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:30:33.637926Z",
     "iopub.status.busy": "2023-11-22T05:30:33.636544Z",
     "iopub.status.idle": "2023-11-22T05:30:33.663838Z",
     "shell.execute_reply": "2023-11-22T05:30:33.661373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MADISON DANE CO RGNL AP       '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "df = pd.DataFrame(cass.execute(\"\"\"\n",
    "    SELECT name from weather.stations\n",
    "    WHERE id='USW00014837'\n",
    "\"\"\"))\n",
    "\n",
    "df[\"name\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a2f623-d931-443a-b371-5ea7628b7ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:30:33.674539Z",
     "iopub.status.busy": "2023-11-22T05:30:33.673949Z",
     "iopub.status.idle": "2023-11-22T05:30:33.735756Z",
     "shell.execute_reply": "2023-11-22T05:30:33.733113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "df = pd.DataFrame(cass.execute(\"\"\"\n",
    "    SELECT token(id) from weather.stations\n",
    "    WHERE id='USC00470273'\n",
    "\"\"\"))\n",
    "\n",
    "df[\"system_token_id\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09f1d71-fb39-4555-bfeb-8be650165f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:30:33.747379Z",
     "iopub.status.busy": "2023-11-22T05:30:33.745965Z",
     "iopub.status.idle": "2023-11-22T05:30:33.754382Z",
     "shell.execute_reply": "2023-11-22T05:30:33.753231Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1bcd0a-0718-4297-a9c8-d3ec86d61428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:30:33.760765Z",
     "iopub.status.busy": "2023-11-22T05:30:33.760312Z",
     "iopub.status.idle": "2023-11-22T05:30:35.877145Z",
     "shell.execute_reply": "2023-11-22T05:30:35.876173Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1476911816910600700"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "sp = subprocess.check_output(\"nodetool ring weather\", stderr=subprocess.STDOUT, shell=True)\n",
    "numbers = re.findall(r'-?\\b\\d{8,}\\b', sp.decode('utf-8'))\n",
    "sorted_numbers = sorted(numbers)\n",
    "closest = 0\n",
    "min = float('inf')\n",
    "for i in range(len(sorted_numbers)):\n",
    "    if (int(sorted_numbers[i]) < min):\n",
    "        min = int(sorted_numbers[i])\n",
    "    if (int(df[\"system_token_id\"][0]) < int(sorted_numbers[i])):\n",
    "        closest = int(sorted_numbers[i])\n",
    "        break\n",
    "if (closest == 0):\n",
    "    closest = min\n",
    "closest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdcea7a-7ce2-4823-8ca9-a5297991319a",
   "metadata": {},
   "source": [
    "## Part 2: Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41626486-b616-4087-b459-ee8e5e67f391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:30:49.895641Z",
     "iopub.status.busy": "2023-11-22T05:30:49.895099Z",
     "iopub.status.idle": "2023-11-22T05:30:50.028873Z",
     "shell.execute_reply": "2023-11-22T05:30:50.026807Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  records.zip\r\n"
     ]
    }
   ],
   "source": [
    "!unzip -n records.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb4424ce-768d-4687-bd59-62fa24550dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:30:50.035465Z",
     "iopub.status.busy": "2023-11-22T05:30:50.034883Z",
     "iopub.status.idle": "2023-11-22T05:30:55.467063Z",
     "shell.execute_reply": "2023-11-22T05:30:55.465877Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+\n",
      "|    station|    date|  TMAX|  TMIN|\n",
      "+-----------+--------+------+------+\n",
      "|USW00014898|20220107| -71.0|-166.0|\n",
      "|USW00014839|20220523| 150.0|  83.0|\n",
      "|USW00014839|20220924| 194.0| 117.0|\n",
      "|USR0000WDDG|20221130| -39.0|-106.0|\n",
      "|USR0000WDDG|20220119| -56.0|-178.0|\n",
      "|USW00014839|20220529| 261.0| 139.0|\n",
      "|USW00014839|20221019|  83.0|  11.0|\n",
      "|USW00014837|20220222| -38.0| -88.0|\n",
      "|USR0000WDDG|20220202|-106.0|-150.0|\n",
      "|USW00014839|20220917| 294.0| 200.0|\n",
      "|USW00014839|20220708| 222.0| 189.0|\n",
      "|USW00014839|20220427|  39.0|   0.0|\n",
      "|USW00014837|20220624| 322.0| 200.0|\n",
      "|USW00014898|20220129| -60.0|-116.0|\n",
      "|USW00014839|20220715| 233.0| 156.0|\n",
      "|USR0000WDDG|20220130| -33.0|-117.0|\n",
      "|USR0000WDDG|20220224| -61.0|-128.0|\n",
      "|USR0000WDDG|20220414|  50.0| -17.0|\n",
      "|USW00014898|20220728| 256.0| 156.0|\n",
      "|USW00014837|20220802| 306.0| 150.0|\n",
      "+-----------+--------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "records_df = (spark.read.format(\"parquet\")\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .load(\"records.parquet\"))\n",
    "# records_df.show()\n",
    "# df1.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").show()\n",
    "filtered_data = records_df.filter((col(\"element\") == \"TMIN\") | (col(\"element\") == \"TMAX\")\n",
    "                 ).groupBy(\"station\", \"date\").pivot(\"element\").agg({\"value\": \"first\"})\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7301b64e-7013-49a8-930f-20e8f7958e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:30:55.473955Z",
     "iopub.status.busy": "2023-11-22T05:30:55.472788Z",
     "iopub.status.idle": "2023-11-22T05:30:55.542678Z",
     "shell.execute_reply": "2023-11-22T05:30:55.541486Z"
    }
   },
   "outputs": [],
   "source": [
    "import station_pb2_grpc\n",
    "import station_pb2\n",
    "import grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f215fa0-5eef-4c15-a323-cc3413a769fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:30:55.550805Z",
     "iopub.status.busy": "2023-11-22T05:30:55.548744Z",
     "iopub.status.idle": "2023-11-22T05:31:27.670889Z",
     "shell.execute_reply": "2023-11-22T05:31:27.669675Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "result_rows = filtered_data.collect()\n",
    "for row in result_rows:\n",
    "    channel = grpc.insecure_channel(\"localhost:5440\")\n",
    "    stub = station_pb2_grpc.StationStub(channel)\n",
    "    resp = stub.RecordTemps(station_pb2.RecordTempsRequest(station=row[\"station\"], \n",
    "                                                        date=row[\"date\"],\n",
    "                                                        tmin=int(row[\"TMIN\"]), \n",
    "                                                        tmax=int(row[\"TMAX\"])))\n",
    "\n",
    "resp2 = stub.StationMax(station_pb2.StationMaxRequest(station=\"USW00014837\"))\n",
    "resp2.tmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f6701-a268-4291-9ddb-44201767309c",
   "metadata": {},
   "source": [
    "## Part 3: Spark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e42bd4f6-e47d-439b-adf5-88f418c30d88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:31:27.676049Z",
     "iopub.status.busy": "2023-11-22T05:31:27.674875Z",
     "iopub.status.idle": "2023-11-22T05:31:30.453806Z",
     "shell.execute_reply": "2023-11-22T05:31:30.452236Z"
    }
   },
   "outputs": [],
   "source": [
    "stations_df = (spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    ".option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\")\n",
    ".option(\"keyspace\", \"weather\") \n",
    ".option(\"table\", \"stations\")\n",
    ".load())\n",
    "\n",
    "stations_df.createOrReplaceTempView(\"stations\")\n",
    "# spark.sql(\"select * from stations limit 3\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d0aa52a-69b6-486f-99b1-37b007e40871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:31:30.461118Z",
     "iopub.status.busy": "2023-11-22T05:31:30.460028Z",
     "iopub.status.idle": "2023-11-22T05:31:31.156496Z",
     "shell.execute_reply": "2023-11-22T05:31:31.155278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2478427e-4898-4a65-a6cc-27de8bd4bd9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:31:31.162588Z",
     "iopub.status.busy": "2023-11-22T05:31:31.162118Z",
     "iopub.status.idle": "2023-11-22T05:31:32.840611Z",
     "shell.execute_reply": "2023-11-22T05:31:32.839363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USW00014839': 89.6986301369863,\n",
       " 'USW00014837': 105.62739726027397,\n",
       " 'USR0000WDDG': 102.06849315068493,\n",
       " 'USW00014898': 102.93698630136986}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "temperature_df = filtered_data.withColumn(\"temp_difference\", col(\"tmax\") - col(\"tmin\"))\n",
    "average_difference_df = temperature_df.groupBy(\"station\").agg({\"temp_difference\": \"avg\"})\n",
    "average_difference_dict = {row[\"station\"]: row[\"avg(temp_difference)\"] for row in average_difference_df.collect()}\n",
    "average_difference_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f67d5b-74b1-4643-b9ba-22ea130c84c3",
   "metadata": {},
   "source": [
    "## Part 4: Disaster Strikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a557a7af-cd58-498c-b851-0b9fb5d40b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:32:06.881451Z",
     "iopub.status.busy": "2023-11-22T05:32:06.880924Z",
     "iopub.status.idle": "2023-11-22T05:32:09.203106Z",
     "shell.execute_reply": "2023-11-22T05:32:09.201707Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.pool:Error attempting to reconnect to 172.28.0.3:9042, scheduling retry in 8.4 seconds: [Errno None] Tried connecting to [('172.28.0.3', 9042)]. Last error: timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\r\n",
      "=======================\r\n",
      "Status=Up/Down\r\n",
      "|/ State=Normal/Leaving/Joining/Moving\r\n",
      "--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \r\n",
      "UN  172.28.0.2  87.74 KiB  16      100.0%            19575424-6d89-4092-8620-c7d3b33225e5  rack1\r\n",
      "UN  172.28.0.4  87.74 KiB  16      100.0%            35a51a44-83e8-483b-8d3d-37097f2de251  rack1\r\n",
      "DN  172.28.0.3  87.74 KiB  16      100.0%            8ff44324-ec21-471a-a971-1fe334012782  rack1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "850c85fb-6827-4231-b77c-ff00e14fb875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:32:09.209595Z",
     "iopub.status.busy": "2023-11-22T05:32:09.209082Z",
     "iopub.status.idle": "2023-11-22T05:32:09.280620Z",
     "shell.execute_reply": "2023-11-22T05:32:09.278989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "channel = grpc.insecure_channel(\"localhost:5440\")\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "resp = stub.StationMax(station_pb2.StationMaxRequest(station=\"USW00014898\"))\n",
    "resp.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acfab62d-a66a-4a3b-b7ed-a55173fbaaa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:32:09.290481Z",
     "iopub.status.busy": "2023-11-22T05:32:09.289382Z",
     "iopub.status.idle": "2023-11-22T05:32:10.107166Z",
     "shell.execute_reply": "2023-11-22T05:32:10.105668Z"
    }
   },
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Exception calling application: bad argument type for built-in operation\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Exception calling application: bad argument type for built-in operation\", grpc_status:2, created_time:\"2023-11-22T05:32:09.306237715+00:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m tmin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      5\u001b[0m tmax \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m----> 6\u001b[0m resp2 \u001b[38;5;241m=\u001b[39m \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecordTemps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecordTempsRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m resp2\u001b[38;5;241m.\u001b[39merror\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1161\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1148\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1155\u001b[0m     (\n\u001b[1;32m   1156\u001b[0m         state,\n\u001b[1;32m   1157\u001b[0m         call,\n\u001b[1;32m   1158\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1159\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[0;32m-> 1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1004\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Exception calling application: bad argument type for built-in operation\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Exception calling application: bad argument type for built-in operation\", grpc_status:2, created_time:\"2023-11-22T05:32:09.306237715+00:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "#q10\n",
    "station = \"test1\"\n",
    "date = \"20232111\"\n",
    "tmin = 3\n",
    "tmax = 30\n",
    "resp2 = stub.RecordTemps(station_pb2.RecordTempsRequest(station=station,\n",
    "                                                       date=date,\n",
    "                                                       tmin=tmin,\n",
    "                                                       tmax=tmax))\n",
    "resp2.error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
